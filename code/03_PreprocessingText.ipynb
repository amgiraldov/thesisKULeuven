{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bf4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "from unidecode import unidecode\n",
    "from collections import Counter\n",
    "from pymongo import MongoClient\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71424e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017')\n",
    "database = client['Thesis']\n",
    "collectionDB = database.get_collection(\"text\")\n",
    "collectionDep_2 = database.get_collection(\"text_dep2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2846be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_excel('../input/stop_words_lemma.xlsx')['stop_lemma'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42fd046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccess_0(texto):\n",
    "    texto = texto.replace('\\n', ' ')\n",
    "    texto = texto.replace('\\t', ' ')\n",
    "    texto = re.sub(' +', ' ', texto)\n",
    "    minusculas = texto.lower()\n",
    "    sin_tildes = unidecode(minusculas)\n",
    "\n",
    "    # Step 1: Eliminar caracteres especiales.\n",
    "    texto_sin_caracteres_especiales = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ\\s]\", \" \", sin_tildes)\n",
    "    # Step 2: Eliminar palabras con más de 20 caracteres o con menos de 2.\n",
    "    palabras = texto_sin_caracteres_especiales.split()\n",
    "    palabras_filtradas = []\n",
    "    for palabra in palabras:\n",
    "        if len(palabra) > 1 and len(palabra) <= 20:\n",
    "            palabras_filtradas.append(palabra)\n",
    "    texto_filtrado = \" \".join(palabras_filtradas) \n",
    "    \n",
    "    if len(texto_filtrado) > 1000000:\n",
    "        sin_tildes = \" \"\n",
    "        for i in range(math.ceil(len(text)/1000000)):\n",
    "            a = math.ceil(len(text)/math.ceil(len(text)/1000000))\n",
    "            text_a = texto_filtrado[i*a:(i+1)*a]\n",
    "            doc = nlp(text_a)\n",
    "            palabras_lematizadas = [token.lemma_ for token in doc]\n",
    "            text_lemma = \" \".join(palabras_lematizadas)\n",
    "            text_unicode = unidecode(text_lemma)\n",
    "            sin_tildes = sin_tildes + \" \" +  text_unicode\n",
    "    else:\n",
    "        # Step 3: Lematizar las palabras.\n",
    "        doc = nlp(texto_filtrado)\n",
    "        palabras_lematizadas = [token.lemma_ for token in doc]\n",
    "        text_lemma = \" \".join(palabras_lematizadas)\n",
    "        sin_tildes = unidecode(text_lemma)     \n",
    "    # Step 3: Eliminar stopwords.\n",
    "    palabras2 = sin_tildes.split()\n",
    "    no_stop = [word for word in palabras2 if word not in stop_words]\n",
    "    texto_sin_stopwords = \" \".join(no_stop)  \n",
    "    return texto_sin_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d5f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca377393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start  -------------------------------\n",
      "12:30:11.254212\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preproccess_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m number \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m texto \u001b[38;5;241m=\u001b[39m \u001b[43mpreproccess_0\u001b[49m(text)\n\u001b[0;32m      8\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      9\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preproccess_0' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Start  -------------------------------\")\n",
    "print(datetime.datetime.now().time())\n",
    "count = 0\n",
    "for document in collectionDB.find():\n",
    "    text = document[\"text\"]\n",
    "    number = document[\"file\"]\n",
    "    texto = preproccess_0(text)\n",
    "    res = dict()\n",
    "    res['version'] = \"0.0.1\"\n",
    "    res['file'] = number\n",
    "    res['text'] = texto\n",
    "    result_object = collectionDep_2.insert_one(res)\n",
    "    \n",
    "print(\"End -------------------------------\")\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7776a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
